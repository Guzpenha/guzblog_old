---
layout: post
title:  ECIR 2019 paper on Document Performance Prediction
date:   2018-12-21 10:00:00
categories: update
comments: true
author: "Guz"
image:
  feature: como.jpg
---

<!-- ![alt](/images/image.png){: .center-image}  -->

In this post I will summarize our recent work “Document Performance Prediction for Automatic Text Classification” that was accepted at ECIR 2019 as a short paper. We study a task we call Document Performance Prediction (DPP) of different classification models, which is inspired by its counterpart in adhoc-retrieval called Query Performance Prediction of different ranking models. 

# Performance Prediction: predicting model effectiveness for specific instances
 
Query performance prediction is a challenging problem in information retrieval. It concerns predicting the effectiveness of a ranking model when there is no relevance information available. The harder a query might be for the ranking model, the lower the predicted performance should be. Good performance predictors are able to effectively predict the actual performance of the model for the given query, measured by an evaluation metric such as nDCG.

![alt]({{ site.baseurl }}/images/queryperformanceprediction.png){: .center-image}

Potential applications for QPP include selecting the best model depending on query features, combining multiple ranking models and requesting more information for potentially poorly formulated queries. 

# Predicting effectiveness of text classifiers

In our paper, we study the task of predicting the performance of text classifiers. As our inputs are now documents, we call this novel task as Document Performance Prediction (DPP). We adapt and propose several performance predictors for this task, including pre-classification and post-classification (also uses the model predictions). We found that the most successful set of features to predict the performance of several text classifiers (XGBoost, KNN, Naive Bayes, Bert, Broof, RandomForest , SVM, Feedforward neural network) were bagging based ones and the ones based solely on the probability distribution outputs of the classifiers. 

**BaggBased**: Predictors from this category are related to QPP that estimate the variance of the retrieved lists. Here we bootstrap the estimators from the bagging-based models and use the variance of their predictions for document classes instead of the scores of top-retrieved documents. BaggCVariance is the std. deviation of each class predicted probability for $n$ (20 in our experiments) random base estimators sampled j (50 in our experiments) times for each classification bagging model m from RF, Bert, Broof and n_estimators = 200 (which is the number of base models compassing the bagging model). BaggQ{25,50,75}C is similar to BaggCVariance, but instead of the std. deviation, we calculate the 25, 50 and 75 quantiles from the class prediction probabilities.PredEntropy is a vector containing the entropy of the base estimators predictions probability distribution for each bagging classification model m from {RF,Bert,Broof}. NumPredC is a vector containing the number of distinct classes (estimated probability not zero) in the base estimators predictions for each bagging classification model m from RF,Bert,Broof.

**ProbPBased**: DPP in this category use the prediction of any classifier, being agnostic to their inductive biases. ProbPred are the probability predictions y_dm of each class for each classification model m, resulting in a vector of dimensionality |M| \times |C|, where M are all the classification models the performances of which are being predicted and C is the target set of classes. ProbPredVar is the standard deviation of probability predictions of each class for each classification model, resulting in a vector of dimensionality |M| * |C|. ProbPBased is the 25, 50 and 75 quantiles of probability predictions of each class for each classification model, resulting in a vector of dimensionality |M| \times |C| \times 3.

Both of them are post-classification predictors, and their parallel in QPP (post-retrieval) are also the state-of-the-art in adhoc retrieval effectiveness prediction.

# Improving classification with effectiveness prediction and ensembling

The expectation that improved QPP techniques would translate to improved retrieval approaches has not yet happened. We studied if we could use DPP in order to improve the classification task itself. We did that by appending the performance predictions (for all models) as additional features in a ensemble that combines the classifiers outputs. Intuitively the stacking layer (we used a Random Forest) might learn to weight the predictions of the classifiers better by using their respectives performance predictions. We expected that predictors with higher correlations, when added as additional features to the ensemble, would lead to higher performance in the classification task. However, only one set of features significantly improved the ensemble performance (the baggind based ones). 

We hypothesize that having a high effectiveness in the performance prediction task by means of correlation is not sufficient for a DPP to improve the classification ensemble, as our empirical results corroborate. In the future we plan to study what is necessary in a performance predictor to improve the ensemble effectiveness, and whether there are better ways of evaluating them.
