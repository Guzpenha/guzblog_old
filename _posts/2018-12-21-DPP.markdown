---
layout: post
title:  ECIR 2019 paper on Document Performance Prediction
date:   2018-12-21 10:00:00
categories: update
comments: true
author: "Guz"
image:
  feature: como.jpg
---

<!-- ![alt](/images/image.png){: .center-image}  -->

In this post I will summarize our recent work “Document Performance Prediction for Automatic Text Classification” that was accepted at ECIR 2019 as a short paper. We study a task we call Document Performance Prediction (DPP) of different classification models, which is inspired by its counterpart in adhoc-retrieval called Query Performance Prediction of different ranking models. 

# Performance Prediction: predicting model effectiveness for specific instances
 
Query performance prediction is a challenging problem in information retrieval. It concerns predicting the effectiveness of a ranking model when there is no relevance information available. The harder a query might be for the ranking model, the lower the predicted performance should be. Good performance predictors are able to effectively predict the actual performance of the model for the given query, measured by an evaluation metric such as nDCG.

![alt]({{ site.baseurl }}/images/queryperformanceprediction.png){: .center-image}

Potential applications for QPP include selecting the best model depending on query features, combining multiple ranking models and requesting more information for potentially poorly formulated queries. 

# Predicting effectiveness of text classifiers

In our paper, we study the task of predicting the performance of text classifiers. As our inputs are now documents, we call this novel task as Document Performance Prediction (DPP).

![alt]({{ site.baseurl }}/images/documentperformanceprediction.png){: .center-image}

We adapt and propose several performance predictors for this task, including pre-classification and post-classification (also uses the model predictions). We found that the most successful set of features to predict the performance of several text classifiers (XGBoost, KNN, Naive Bayes, Bert, Broof, RandomForest , SVM, Feedforward neural network) were bagging based ones and the ones based solely on the probability distribution outputs of the classifiers. Both of them are post-classification predictors, and their parallel in QPP (post-retrieval) are also the state-of-the-art in adhoc retrieval effectiveness prediction.

# Improving classification with effectiveness prediction and ensembling

The expectation that improved QPP techniques would translate to improved retrieval approaches has not yet happened. We studied if we could use DPP in order to improve the classification task itself. We did that by appending the performance predictions (for all models) as additional features in a ensemble that combines the classifiers outputs. Intuitively the stacking layer (we used a Random Forest) might learn to weight the predictions of the classifiers better by using their respectives performance predictions. We expected that predictors with higher correlations, when added as additional features to the ensemble, would lead to higher performance in the classification task. However, only one set of features significantly improved the ensemble performance (the baggind based ones). 

We hypothesize that having a high effectiveness in the performance prediction task by means of correlation is not sufficient for a DPP to improve the classification ensemble, as our empirical results corroborate. In the future we plan to study what is necessary in a performance predictor to improve the ensemble effectiveness, and whether there are better ways of evaluating them.
