---
layout: post
title:  How well will our text classifier perform? Using performance prediction to enhance ensembles
date:   2018-12-21 10:00:00
categories: update
comments: true
author: "Guz"
image:
  feature: como.jpg
---

<!-- ![alt](/images/image.png){: .center-image}  -->

In this post I will summarize our recent work “Document Performance Prediction for Automatic Text Classification” that was accepted at ECIR 2019 as a short paper. We study a task we call Document Performance Prediction (DPP) of different classification models, which is inspired by its counterpart in adhoc-retrieval called Query Performance Prediction of different ranking models. 

# Performance Prediction: predicting model effectiveness for specific instances
 
Query performance prediction is a challenging problem in information retrieval. It concerns predicting the effectiveness of a ranking model when there is no relevance information available. The harder a query might be for the ranking model, the lower the predicted performance should be. Good performance predictors are able to effectively predict the actual performance of the model for the given query, measured by an evaluation metric such as nDCG.

![alt]({{ site.baseurl }}/images/queryperformanceprediction.png){: .center-image}

Potential applications for QPP include selecting the best model depending on query features, combining multiple ranking models and requesting more information for potentially poorly formulated queries. 

# Predicting effectiveness of text classifiers

In our paper, we study the task of predicting the performance of text classifiers. As our inputs are now documents, we call this novel task as Document Performance Prediction (DPP).

![alt]({{ site.baseurl }}/images/documentperformanceprediction.png){: .center-image}

We adapt and propose several performance predictors for this task, including pre-classification and post-classification (also uses the model predictions). We found that the most successful set of features to predict the performance of several text classifiers (XGBoost, KNN, Naive Bayes, Bert, Broof, RandomForest , SVM, Feedforward neural network) were bagging based ones and the ones based solely on the probability distribution outputs of the classifiers. Both of them are post-classification predictors, and their parallel in QPP (post-retrieval) are also the state-of-the-art in adhoc retrieval effectiveness prediction.

# Improving classification with effectiveness prediction

The expectation that improved QPP techniques would translate to improved retrieval approaches has not yet happened. We studied if we could use DPP in order to improve the classification task itself. We did that by appending the performance predictions (for all models) as additional features in a ensemble that combines the classifiers outputs. Intuitively the stacking layer (we used a Random Forest) might learn to weight the predictions of the classifiers better by using their respectives performance predictions. We expected that predictors with higher correlations, when added as additional features to the ensemble, would lead to higher performance in the classification task. However, only one set of features significantly improved the ensemble performance (the baggind based ones). 

We hypothesize that having a high effectiveness in the performance prediction task by means of correlation is not sufficient for a DPP to improve the classification ensemble, as our empirical results corroborate. In the future we plan to study what is necessary in a performance predictor to improve the ensemble effectiveness, and whether there are better ways of evaluating them.


# Reviews 
The most unfavorable was the third reviewer, which argues that we should not have used the average effectiveness of the performance predictors over all classifiers, as we should be interested in finding the best performance predictor for a fixed classifier, and not the best performance predictor on average (regarding classifiers). I agree that having a table for each of the M classifiers would be more insightful, by helping us understand which set of performance predictors would be best for each of the classification models (unfortunately we did not find a better way of displaying this information within the space of a short paper). On the other hand, we argue that using the average over classifiers is in fact a good way of comparing the performance predictors, when we are interested in combining multiple classification models with the aid of the performance predictions (as we did in Section 3), they should be robust to predict the performance of the multiple models being combined. That said, the Table 3 was misinterpreted by the reviewer, as we are displaying the metrics for the classification task, where the model is the ensemble that is combining multiple classifiers, which is most likely our fault and we hope that we made this more clear and detailed in the experimental setup for the camera-ready paper.

### Review 1

Recommendation: 1 (Solid work (Accept): I would like to see it accepted.)

Evaluations: 4|3|4|4|3|5|4

This is a very straight-forward paper. The authors in practice evaluate different feature representations for the task of document performance prediction in a text classification scenario. The authors follow a well-established structure e.g. motivation, research questions, experiments and so on. This makes the content easy to understand and follow. While the paper seems to be the first to consider the DPP task on text classification, it does not propose anything new (methods, metrics). Its novelty derives from simply applying the task to a different context. This is not necessarily bad. I feel that this paper acts a starting point for further research, such as "why predictors with high correlations on the document performance prediction task do not necessarily translate into improved text classification" (as the authors mention). At the same time, this paper should insite some research on why certain features work better than others for this specific textual context. In other words, instead of using text classification as simply "another" domain for DPP, I would love to see the application of domain knowledge to the problem. One thing that I am definitely missing, is the 'related work' section. While the authors argue that their work is novel (the application at least), I am confident that some works would be at least tangential (maybe the evaluation of the same features in a slightly different context). In general, the paper is successful in providing what it promises. The research questions are clear and the experiments as well. But due to the lack of a very novel component, I would suggest the authors to spend more time on the "future work" part. I need to be more motivated that this paper is a strong foundation for more interesting things.

### Review 2

Recommendation: 2 (Exciting (Strong accept): I would fight to get it accepted.)

Evaluations: 4|4|4|4|4|4|4

The authors propose a model for a novel task of Document Performance Prediction (DPP), which is defined as predicting the effectiveness of a text classifier for a given document, when labelled data is not available.  Authors propose several methods inspired by the literature from Query Performance Prediction (QPP) task. Specifically, they propose two methods in Pre-classification DPP setting, where only the document content and corpus statistics are used, and two methods in Post-classification DPP setting, where document content along with classifier's probability outputs are used. Post-classification DPP method outperform their pre-classification counterpart. The paper is generally well-written and easy to follow. I have a few comments, addressed below: 1) In the d-latent method from pre-classification DPP setup, the document representation seems to be a concatenation of pooled token representation. Document in a corpus have different number of tokens, which can result in varied document representation length. How is this handled? 2) In DistBased method from Post-classification DPP, a distance between the document and the category centroid is computed. How is the category centroid computed? It's not clear. Overall, a solid paper tackling an interesting problem.

### Review 3

Recommendation: 0 (Worthy (Weak accept): Accept if there is room.)

Evaluations: 3|2|3|3|4|3|3

Inspired by Query performance prediction (QPP) methods, this work proposes methods for document performance prediction (DPP) that aim at predicting the performance of automatic text classifiers. Strengths: A topic relevant to ECIR is addressed. In the experiments,  relevant learners and data sets are used. The paper is well written in terms of language. Weaknesses:- The results are not fully credible, given that the results tables are not necessarily fully meaningful, as explained below. If I understand your setting correctly, in your experiment the goal is to evaluate the performance of d x m, for d in D and m in M, where D is a set of documents described by a given feature set d (one of the possible approaches described in Sec. 2.1 and 2.2), and M is a set of classifiers m (one of the classifiers fXGBoost, KNN, NaiveBayes, Bert, Broof, RandomForest, SVM, MultiLayerPerceptron). If the results in the table are composed by evaluating DPP by performing 5-fold cross-validation as follows: In each round, four folds serve as the training corpus T and the remaining fold is used to calculate the correlation between the predicted and actual performance of each classification model m, averaged across all models in M, abd that the mean of the average correlation obtained by each DPP across the five test folds is reported. I argue that this is not a fully valid setting for DPP performance evaluation. Namely, why should you be interested in selecting the best document description method which would achieve best AVERAGE performance over all classifiers ? In current Machine Learning research, in particular in recent AutoML approaches, it is clearly the case that the choice of a method for data preprocessing (i.e. the method for selecting the appropriate feature set to describe the documents in your case) depends on the classifier, and not on the average classifier. It does not make sense to find the best feature set describing the documents averaging over different classifiers, the right question to ask is which is the best feature set for the given classifier. So, in my opinion the results in Table 1, the approach taken to present the results of the best-performing DPP in each of the categories described in Sec. 2.1 and 2.2 is not fully appropriate. Next, I am also not quite sure about Table 3, but probably it is not OK, as probably (this is not explicitly described) it again shows the averages over different classifiers. On the other hand, I believe that the results reported in Table 2 for the random forest regressor are correct. If I understand correctly, in this table you evaluate the performance of d x m, for d in D and m in M, where D is a set of documents described by a given feature set d (one of the possible approaches described in Sec. 2.1 and 2.2), and M is a single ensemble classifier m (RandomForest). Additional hint for improving the presentation of the paper: Probably it would be helpful to the reader if you presented your experimental settings in a more detailed way (see text above), which would ensure that the experimental setting is not misunderstood and which would enable the reimplementation and comparison of results. Other comments: Some references are missing (the corpora used, WordNet, similarity metrics).
