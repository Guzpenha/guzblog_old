---
layout: post
title:  ECIR 2019 paper on Document Performance Prediction
date:   2018-12-21 10:00:00
categories: update
comments: true
author: "Guz"
image:
  feature: como.jpg
---

<!-- ![alt](/images/image.png){: .center-image}  -->

In this post I will summarize our recent work “Document Performance Prediction for Automatic Text Classification” that was accepted at ECIR 2019 as a short paper. We study a task we call Document Performance Prediction (DPP) of different classification models, which is inspired by its counterpart in adhoc-retrieval called Query Performance Prediction of different ranking models. 

# Performance Prediction: predicting model effectiveness for specific instances
 
Query performance prediction is a challenging problem in information retrieval. It concerns predicting the effectiveness of a ranking model when there is no relevance information available. The harder a query might be for the ranking model, the lower the predicted performance should be. Good performance predictors are able to effectively predict the actual performance of the model for the given query, measured by an evaluation metric such as nDCG.

![alt]({{ site.baseurl }}/images/queryperformanceprediction.png){: .center-image}

Potential applications for QPP include selecting the best model depending on query features, combining multiple ranking models and requesting more information for potentially poorly formulated queries. 

# Predicting effectiveness of text classifiers

In our paper, we study the task of predicting the performance of text classifiers. As our inputs are now documents, we call this novel task as Document Performance Prediction (DPP).

![alt]({{ site.baseurl }}/images/documentperformanceprediction.png){: .center-image}

We adapt and propose several performance predictors for this task, including pre-classification and post-classification (also uses the model predictions). We found that the most successful set of features to predict the performance of several text classifiers (XGBoost, KNN, Naive Bayes, Bert, Broof, RandomForest , SVM, Feedforward neural network) were bagging based ones and the ones based solely on the probability distribution outputs of the classifiers. Both of them are post-classification predictors, and their parallel in QPP (post-retrieval) are also the state-of-the-art in adhoc retrieval effectiveness prediction.

# Improving classification with effectiveness prediction

The expectation that improved QPP techniques would translate to improved retrieval approaches has not yet happened. We studied if we could use DPP in order to improve the classification task itself. We did that by appending the performance predictions (for all models) as additional features in a ensemble that combines the classifiers outputs. Intuitively the stacking layer (we used a Random Forest) might learn to weight the predictions of the classifiers better by using their respectives performance predictions. We expected that predictors with higher correlations, when added as additional features to the ensemble, would lead to higher performance in the classification task. However, only one set of features significantly improved the ensemble performance (the baggind based ones). 

We hypothesize that having a high effectiveness in the performance prediction task by means of correlation is not sufficient for a DPP to improve the classification ensemble, as our empirical results corroborate. In the future we plan to study what is necessary in a performance predictor to improve the ensemble effectiveness, and whether there are better ways of evaluating them.


# Reviews 
The most unfavorable was the third reviewer, which argues that we should not have used the average effectiveness of the performance predictors over all classifiers, as we should be interested in finding the best performance predictor for a fixed classifier, and not the best performance predictor on average (regarding classifiers). I agree that having a table for each of the M classifiers would be more insightful, by helping us understand which set of performance predictors would be best for each of the classification models (unfortunately we did not find a better way of displaying this information within the space of a short paper). On the other hand, we argue that using the average over classifiers is in fact a good way of comparing the performance predictors, when we are interested in combining multiple classification models with the aid of the performance predictions (as we did in Section 3), they should be robust to predict the performance of the multiple models being combined. That said, the Table 3 was misinterpreted by the reviewer, as we are displaying the metrics for the classification task, where the model is the ensemble that is combining multiple classifiers, which is most likely our fault and we hope that we made this more clear and detailed in the experimental setup for the camera-ready paper.

